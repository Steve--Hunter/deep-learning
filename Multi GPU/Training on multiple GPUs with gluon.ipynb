{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on multiple GPUs with gluon\n",
    "from https://gluon.mxnet.io/chapter07_distributed-learning/multiple-gpus-gluon.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd, gluon, autograd\n",
    "net = gluon.nn.Sequential(prefix='cnn_')\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Conv2D(channels=20, kernel_size=3, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "    net.add(gluon.nn.Conv2D(channels=50, kernel_size=5, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "    net.add(gluon.nn.Flatten())\n",
    "    net.add(gluon.nn.Dense(128, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(10))\n",
    "\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_COUNT = 4 # increase if you have more\n",
    "ctx = [mx.gpu(i) for i in range(GPU_COUNT)]\n",
    "net.collect_params().initialize(ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[-0.00481915  0.0019142   0.02079279 -0.01032627  0.01591925 -0.00201489\n",
      "  -0.00969937 -0.01089183 -0.00311435  0.00096905]\n",
      " [ 0.00226964 -0.00863827  0.01155043 -0.03112907  0.04687939 -0.01447407\n",
      "  -0.00345835 -0.01686897 -0.00883059 -0.0048196 ]\n",
      " [-0.01494183  0.00641099  0.02740017 -0.01331076  0.01718415 -0.02049803\n",
      "  -0.00956278 -0.0193483   0.0079064  -0.01144795]\n",
      " [-0.00765511  0.00055096  0.00661376 -0.01600863  0.02302844 -0.00434789\n",
      "  -0.01212487 -0.01363104  0.00415654 -0.00635324]]\n",
      "<NDArray 4x10 @gpu(0)>\n",
      "\n",
      "[[-0.00461744  0.01375779  0.03823002 -0.03126055  0.0333534  -0.01731854\n",
      "  -0.00492363 -0.01554649 -0.00743299 -0.00639034]\n",
      " [-0.0091598  -0.00684692  0.01705891 -0.03048797  0.02992028 -0.01165187\n",
      "  -0.01089531 -0.01976818  0.00234484  0.00295688]\n",
      " [-0.01501885  0.00427208  0.008796   -0.0090834   0.0166386  -0.00924412\n",
      "  -0.00995954  0.00503164 -0.00183729 -0.00526959]\n",
      " [-0.0001063  -0.010359    0.02876046 -0.02604114  0.02878538 -0.01912549\n",
      "  -0.00995447 -0.00781563  0.00229009 -0.00901604]]\n",
      "<NDArray 4x10 @gpu(1)>\n",
      "\n",
      "[[ -9.55712702e-03   8.14154744e-03   1.41310692e-02  -7.50806229e-03\n",
      "    1.21279778e-02  -4.99197841e-03  -5.68637112e-03   2.85031763e-03\n",
      "    1.14737265e-03  -6.18950324e-03]\n",
      " [  3.83510720e-03   6.30928250e-03   1.70313437e-02  -2.46480182e-02\n",
      "    1.90254450e-02  -1.46575542e-02  -9.08921566e-03  -2.55176313e-02\n",
      "   -4.43728313e-05  -8.75856448e-03]\n",
      " [ -1.05991373e-02  -3.37098422e-03   1.60193965e-02  -2.14238968e-02\n",
      "    2.82504763e-02  -1.63856265e-03  -9.25079454e-03  -3.21536465e-03\n",
      "   -7.85421580e-03  -1.22634796e-02]\n",
      " [ -2.46961764e-03   9.76554770e-03   1.76086780e-02  -1.32546732e-02\n",
      "    2.08108369e-02  -1.55222928e-03  -6.37652725e-03  -2.04339717e-02\n",
      "   -1.30823243e-03  -7.51803629e-03]]\n",
      "<NDArray 4x10 @gpu(2)>\n",
      "\n",
      "[[-0.00286213 -0.00973572  0.02996843 -0.02276067  0.02720474 -0.00222515\n",
      "  -0.01257315  0.00740586 -0.00619914 -0.01214778]\n",
      " [-0.00283094 -0.00661631  0.02154266 -0.02816908  0.02662642 -0.00898186\n",
      "   0.00596539 -0.02482983 -0.00090994 -0.00634332]\n",
      " [-0.01062217  0.00885265  0.01324536 -0.00655865  0.00766904 -0.00662809\n",
      "  -0.00768047 -0.00216596 -0.00478933 -0.00731292]\n",
      " [ 0.01057165  0.00515975  0.03281771 -0.03074188  0.03171781 -0.01809868\n",
      "   0.00518375 -0.02114278 -0.00909929 -0.00114449]]\n",
      "<NDArray 4x10 @gpu(3)>\n"
     ]
    }
   ],
   "source": [
    "from mxnet.test_utils import get_mnist\n",
    "mnist = get_mnist()\n",
    "batch = mnist['train_data'][0:GPU_COUNT*4, :]\n",
    "data = gluon.utils.split_and_load(batch, ctx)\n",
    "print(net(data[0]))\n",
    "print(net(data[1]))\n",
    "print(net(data[2]))\n",
    "print(net(data[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== channel 0 of the first conv on gpu(0) ===\n",
      "[[[ 0.0068339   0.01299825  0.0301265 ]\n",
      "  [ 0.04819721  0.01438687  0.05011239]\n",
      "  [ 0.00628365  0.04861524 -0.01068833]]]\n",
      "<NDArray 1x3x3 @gpu(0)>\n",
      "=== channel 0 of the first conv on gpu(1) ===\n",
      "[[[ 0.0068339   0.01299825  0.0301265 ]\n",
      "  [ 0.04819721  0.01438687  0.05011239]\n",
      "  [ 0.00628365  0.04861524 -0.01068833]]]\n",
      "<NDArray 1x3x3 @gpu(1)>\n",
      "=== channel 0 of the first conv on gpu(2) ===\n",
      "[[[ 0.0068339   0.01299825  0.0301265 ]\n",
      "  [ 0.04819721  0.01438687  0.05011239]\n",
      "  [ 0.00628365  0.04861524 -0.01068833]]]\n",
      "<NDArray 1x3x3 @gpu(2)>\n",
      "=== channel 0 of the first conv on gpu(3) ===\n",
      "[[[ 0.0068339   0.01299825  0.0301265 ]\n",
      "  [ 0.04819721  0.01438687  0.05011239]\n",
      "  [ 0.00628365  0.04861524 -0.01068833]]]\n",
      "<NDArray 1x3x3 @gpu(3)>\n"
     ]
    }
   ],
   "source": [
    "weight = net.collect_params()['cnn_conv0_weight']\n",
    "\n",
    "for c in ctx:\n",
    "    print('=== channel 0 of the first conv on {} ==={}'.format(\n",
    "        c, weight.data(ctx=c)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== grad of channel 0 of the first conv2d on gpu(0) ===\n",
      "[[[-0.05446478 -0.06920426 -0.03950822]\n",
      "  [-0.0742496  -0.06848756 -0.02138805]\n",
      "  [-0.00942846 -0.02732194 -0.02662538]]]\n",
      "<NDArray 1x3x3 @gpu(0)>\n",
      "=== grad of channel 0 of the first conv2d on gpu(1) ===\n",
      "[[[ 0.13215259  0.13300328  0.09848808]\n",
      "  [ 0.10377123  0.09225045  0.09771539]\n",
      "  [ 0.0414196   0.08219814  0.14231277]]]\n",
      "<NDArray 1x3x3 @gpu(1)>\n",
      "=== grad of channel 0 of the first conv2d on gpu(2) ===\n",
      "[[[-0.15555602 -0.12390864  0.00930843]\n",
      "  [-0.17069009 -0.10882725  0.00049725]\n",
      "  [-0.08165155 -0.04167913  0.01819187]]]\n",
      "<NDArray 1x3x3 @gpu(2)>\n",
      "=== grad of channel 0 of the first conv2d on gpu(3) ===\n",
      "[[[-0.12295836 -0.08860554 -0.08663689]\n",
      "  [-0.08101771 -0.10529852 -0.11382867]\n",
      "  [-0.04248851 -0.08572525 -0.07257522]]]\n",
      "<NDArray 1x3x3 @gpu(3)>\n"
     ]
    }
   ],
   "source": [
    "def forward_backward(net, data, label):\n",
    "    with autograd.record():\n",
    "        losses = [loss(net(X), Y) for X, Y in zip(data, label)]\n",
    "    for l in losses:\n",
    "        l.backward()\n",
    "\n",
    "label = gluon.utils.split_and_load(mnist['train_label'][0:16], ctx) # Changed to 0:16 for 4 GPUs\n",
    "forward_backward(net, data, label)\n",
    "for c in ctx:\n",
    "    print('=== grad of channel 0 of the first conv2d on {} ==={}'.format(\n",
    "        c, weight.grad(ctx=c)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on [gpu(0)]\n",
      "Batch size is 64\n",
      "Epoch 0, training time = 3.6 sec\n",
      "         validation accuracy = 0.9685\n",
      "Epoch 1, training time = 3.5 sec\n",
      "         validation accuracy = 0.9814\n",
      "Epoch 2, training time = 3.5 sec\n",
      "         validation accuracy = 0.9862\n",
      "Epoch 3, training time = 3.4 sec\n",
      "         validation accuracy = 0.9848\n",
      "Epoch 4, training time = 3.5 sec\n",
      "         validation accuracy = 0.9853\n",
      "Running on [gpu(0), gpu(1), gpu(2), gpu(3)]\n",
      "Batch size is 256\n",
      "Epoch 0, training time = 2.4 sec\n",
      "         validation accuracy = 0.9488\n",
      "Epoch 1, training time = 2.3 sec\n",
      "         validation accuracy = 0.9693\n",
      "Epoch 2, training time = 2.4 sec\n",
      "         validation accuracy = 0.9763\n",
      "Epoch 3, training time = 2.4 sec\n",
      "         validation accuracy = 0.9801\n",
      "Epoch 4, training time = 2.3 sec\n",
      "         validation accuracy = 0.9844\n"
     ]
    }
   ],
   "source": [
    "from mxnet.io import NDArrayIter\n",
    "from time import time\n",
    "\n",
    "def train_batch(batch, ctx, net, trainer):\n",
    "    # split the data batch and load them on GPUs\n",
    "    data = gluon.utils.split_and_load(batch.data[0], ctx)\n",
    "    label = gluon.utils.split_and_load(batch.label[0], ctx)\n",
    "    # compute gradient\n",
    "    forward_backward(net, data, label)\n",
    "    # update parameters\n",
    "    trainer.step(batch.data[0].shape[0])\n",
    "\n",
    "def valid_batch(batch, ctx, net):\n",
    "    data = batch.data[0].as_in_context(ctx[0])\n",
    "    pred = nd.argmax(net(data), axis=1)\n",
    "    return nd.sum(pred == batch.label[0].as_in_context(ctx[0])).asscalar()\n",
    "\n",
    "def run(num_gpus, batch_size, lr):\n",
    "    # the list of GPUs will be used\n",
    "    ctx = [mx.gpu(i) for i in range(num_gpus)]\n",
    "    print('Running on {}'.format(ctx))\n",
    "\n",
    "    # data iterator\n",
    "    mnist = get_mnist()\n",
    "    train_data = NDArrayIter(mnist[\"train_data\"], mnist[\"train_label\"], batch_size)\n",
    "    valid_data = NDArrayIter(mnist[\"test_data\"], mnist[\"test_label\"], batch_size)\n",
    "    print('Batch size is {}'.format(batch_size))\n",
    "\n",
    "    net.collect_params().initialize(force_reinit=True, ctx=ctx)\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "    for epoch in range(5):\n",
    "        # train\n",
    "        start = time()\n",
    "        train_data.reset()\n",
    "        for batch in train_data:\n",
    "            train_batch(batch, ctx, net, trainer)\n",
    "        nd.waitall()  # wait until all computations are finished to benchmark the time\n",
    "        print('Epoch %d, training time = %.1f sec'%(epoch, time()-start))\n",
    "\n",
    "        # validating\n",
    "        valid_data.reset()\n",
    "        correct, num = 0.0, 0.0\n",
    "        for batch in valid_data:\n",
    "            correct += valid_batch(batch, ctx, net)\n",
    "            num += batch.data[0].shape[0]\n",
    "        print('         validation accuracy = %.4f'%(correct/num))\n",
    "\n",
    "run(1, 64, .3)\n",
    "run(GPU_COUNT, 64*GPU_COUNT, .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
